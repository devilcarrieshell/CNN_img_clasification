{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicația practică la disciplina Prelucrarea imaginilor în realitatea virtuală:\n",
    "# 1.Definirea problemei:\n",
    "Dezvoltarea unei rețele neuronale convoluționale; să antrenez un model pentru a recunoaște imaginile din dataset-ul CIFAR10 pe 10 categorii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## configurarea stackului: \n",
    "## un distributiv python sau anaconda; am folosit VSCode cu python 3.9 și extesia Jupyter\n",
    "#update python la versiunea 3.11\n",
    "## install pytorch torchvision cpuonly -c pytorch (dacă nu avem GPU cu nucleu CUDA )\n",
    "## pip install pytorch torchvision pytorch-cuda -c pytorch -c nvidia (GPU nvidia cu nuclee CUDA)\n",
    "## install streamlit - pentru implementarea moselului pe platforma și a demonstra funcționalitatea "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Colectarea datelor: import biblioteca PyTorch, modulul pentru rețea neuronală, biblioteca torchvision, NumPy și modulul pyplot din biblioteca matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "#device=torch.device(\"cpu:0\") #daca rulezi pe cpu\n",
    "device=torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ## nu nu lucrează oe toate dispozitivile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pregătirea datelor: definirea unei transformări a datelor care convertește imaginile de intrare în tensori PyTorch și le normalizează, setarea dimensiunii lotului și crearea încărcătoarelor de date pentru setul de date CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4 # atent la training pe procesor\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/tmp/cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/tmp/cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck') #de făcut un dataset pentru încercări"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Algoritmul: definirea unei clase ConvModel care extinde clasa PyTorch nn.Module. Această clasă definește arhitectura modelului CNN, care constă din două straturi convoluționale urmate de două straturi complet conectate. Metoda forward implementează trecerea înainte a modelului."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvModel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels= 32,\n",
    "            kernel_size=3, \n",
    "            padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2, \n",
    "            stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, \n",
    "            out_channels=64, \n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=64 * 8 * 8, \n",
    "            out_features=512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=512, \n",
    "                             out_features=10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)# de verificat dimensiunea la flatten; mai optim ar fi\n",
    "        #x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = ConvModel().to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Optimizarea modelului: definirea funcției de pierdere (cross-entropy) și a optimizatorului (coborâre stocastică a gradientului)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Formarea modelului: formarea modelului timp de două epoci utilizând datele de formare. În fiecare epocă, iterez peste loturile din datele de instruire, calculează trecerile înainte și înapoi și actualizează ponderile modelului utilizând optimizatorul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.950\n",
      "[1,  4000] loss: 1.563\n",
      "[1,  6000] loss: 1.392\n",
      "[1,  8000] loss: 1.319\n",
      "[1, 10000] loss: 1.214\n",
      "[1, 12000] loss: 1.165\n",
      "[2,  2000] loss: 1.057\n",
      "[2,  4000] loss: 0.991\n",
      "[2,  6000] loss: 0.989\n",
      "[2,  8000] loss: 0.959\n",
      "[2, 10000] loss: 0.921\n",
      "[2, 12000] loss: 0.908\n",
      "[3,  2000] loss: 0.771\n",
      "[3,  4000] loss: 0.790\n",
      "[3,  6000] loss: 0.761\n",
      "[3,  8000] loss: 0.750\n",
      "[3, 10000] loss: 0.760\n",
      "[3, 12000] loss: 0.731\n",
      "[4,  2000] loss: 0.569\n",
      "[4,  4000] loss: 0.579\n",
      "[4,  6000] loss: 0.591\n",
      "[4,  8000] loss: 0.592\n",
      "[4, 10000] loss: 0.602\n",
      "[4, 12000] loss: 0.587\n",
      "[5,  2000] loss: 0.379\n",
      "[5,  4000] loss: 0.398\n",
      "[5,  6000] loss: 0.406\n",
      "[5,  8000] loss: 0.433\n",
      "[5, 10000] loss: 0.447\n",
      "[5, 12000] loss: 0.448\n",
      "[6,  2000] loss: 0.220\n",
      "[6,  4000] loss: 0.249\n",
      "[6,  6000] loss: 0.261\n",
      "[6,  8000] loss: 0.273\n",
      "[6, 10000] loss: 0.292\n",
      "[6, 12000] loss: 0.317\n",
      "[7,  2000] loss: 0.126\n",
      "[7,  4000] loss: 0.137\n",
      "[7,  6000] loss: 0.156\n",
      "[7,  8000] loss: 0.176\n",
      "[7, 10000] loss: 0.188\n",
      "[7, 12000] loss: 0.204\n",
      "[8,  2000] loss: 0.083\n",
      "[8,  4000] loss: 0.081\n",
      "[8,  6000] loss: 0.108\n",
      "[8,  8000] loss: 0.114\n",
      "[8, 10000] loss: 0.129\n",
      "[8, 12000] loss: 0.121\n",
      "[9,  2000] loss: 0.052\n",
      "[9,  4000] loss: 0.063\n",
      "[9,  6000] loss: 0.062\n",
      "[9,  8000] loss: 0.090\n",
      "[9, 10000] loss: 0.092\n",
      "[9, 12000] loss: 0.104\n",
      "[10,  2000] loss: 0.048\n",
      "[10,  4000] loss: 0.047\n",
      "[10,  6000] loss: 0.056\n",
      "[10,  8000] loss: 0.072\n",
      "[10, 10000] loss: 0.068\n",
      "[10, 12000] loss: 0.090\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for epoch in range(10):  # parcurgerea în buclă a setului de date de n ori\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "            #stats: pe gtx 1660ti a fost 23min\n",
    "            #rtx 3060ti 8 min a durat \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluare: evaluarea performanței modelului antrenat pe datele de testare prin calcularea preciziei modelului. Setați modelul în modul de evaluare pentru a dezactiva abandonul și normalizarea loturilor, apoi iterați pe datele de testare pentru a calcula precizia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precizia desusă în urma încercărilor făcute în baza a 10000 imagini: 72.640000 %\n"
     ]
    }
   ],
   "source": [
    "#test precizie - obiectiv mai mult de 70 la sută\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Precizia desusă în urma încercărilor făcute în baza a 10000 imagini: %f %%' % (\n",
    "    100 * correct / total))\n",
    "#rev 2 - 73,4%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Această rețea neuronală convoluțională (CNN) este un model de învățare profundă care este antrenat pe setul de date CIFAR-10 pentru clasificarea imaginilor. Setul de date CIFAR-10 constă din 60 000 de imagini color 32x32 în 10 clase, cu 6 000 de imagini pe clasă. Arhitectura modelului constă din două straturi convoluționale și două straturi complet conectate. Primul strat convoluțional are 32 de filtre de dimensiune 3x3, iar al doilea strat convoluțional are 64 de filtre de dimensiune 3x3. Ambele straturi convoluționale sunt urmate de un strat max pooling care reduce dimensiunile spațiale ale hărților de caracteristici. Ieșirea celui de-al doilea strat max pooling este aplatizată și introdusă în două straturi complet conectate, primul având 512 neuroni, iar al doilea având 10 neuroni (câte unul pentru fiecare clasă din setul de date). Funcția de activare ReLU este utilizată după fiecare strat convoluțional și complet conectat, cu excepția ultimului, care utilizează o funcție de activare softmax pentru a emite probabilitățile ca imaginea de intrare să aparțină fiecărei clase. Modelul este antrenat utilizând metoda stochastic gradient descent (SGD) cu o rată de învățare de 0,001 și un momentum de 0,9, iar funcția de pierdere a entropiei încrucișate este utilizată ca obiectiv de optimizare.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. implementare: am folosit biblioteca Streamlit, iar acum pot crea o interfață care să permită utilizatorilor să interacționeze cu modelul dvs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'cifar10_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
