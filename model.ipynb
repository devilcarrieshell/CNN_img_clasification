{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Problem definition:\n",
    "I want to train a convolutional neural network, train a model to classify images from the CIFAR10 dataset into 10 categories.\n",
    "\n",
    "This is second implementation of my model. First was on MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to run this program, make sure that you have install: \n",
    "## anaconda or another python distribution \n",
    "## install pytorch torchvision cpuonly -c pytorch (if you dont have CUDA-capable GPU )\n",
    "## install pytorch torchvision pytorch-cuda -c pytorch -c nvidia (if you have GPU with CUDA )\n",
    "## install streamlit\n",
    "## conda install -c conda-forge python-telegram-bot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data collrcting: importing PyTorch, its neural network module, the torchvision library, NumPy, and the pyplot module from the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "#device=torch.device(\"cpu:0\")\n",
    "device=torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data preparation: defining a data transformation that converts the input images to PyTorch tensors and normalizes them, setting the batch size, and creating data loaders for the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4 # Cannot be too much when training on cpu\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/tmp/cifar10', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/tmp/cifar10', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Alhorithm: defining a ConvModel class that extends the PyTorch nn.Module class. This class defines the architecture of the CNN model, which consists of two convolutional layers followed by two fully connected layers. The forward() method implements the forward pass of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvModel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels= 32,\n",
    "            kernel_size=3, \n",
    "            padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2, \n",
    "            stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, \n",
    "            out_channels=64, \n",
    "            kernel_size=3,\n",
    "            padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=64 * 8 * 8, \n",
    "            out_features=512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=512, \n",
    "                             out_features=10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = ConvModel().to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Model optimization: defining the loss function (cross-entropy) and the optimizer (stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model train: training the model for two epochs using the training data. In each epoch, you are iterating over the batches in the training data, computing the forward and backward passes, and updating the weights of the model using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.916\n",
      "[1,  4000] loss: 1.529\n",
      "[1,  6000] loss: 1.380\n",
      "[1,  8000] loss: 1.271\n",
      "[1, 10000] loss: 1.207\n",
      "[1, 12000] loss: 1.138\n",
      "[2,  2000] loss: 1.028\n",
      "[2,  4000] loss: 0.990\n",
      "[2,  6000] loss: 0.958\n",
      "[2,  8000] loss: 0.934\n",
      "[2, 10000] loss: 0.892\n",
      "[2, 12000] loss: 0.898\n",
      "[3,  2000] loss: 0.749\n",
      "[3,  4000] loss: 0.772\n",
      "[3,  6000] loss: 0.750\n",
      "[3,  8000] loss: 0.735\n",
      "[3, 10000] loss: 0.722\n",
      "[3, 12000] loss: 0.721\n",
      "[4,  2000] loss: 0.552\n",
      "[4,  4000] loss: 0.555\n",
      "[4,  6000] loss: 0.556\n",
      "[4,  8000] loss: 0.562\n",
      "[4, 10000] loss: 0.577\n",
      "[4, 12000] loss: 0.575\n",
      "[5,  2000] loss: 0.346\n",
      "[5,  4000] loss: 0.364\n",
      "[5,  6000] loss: 0.390\n",
      "[5,  8000] loss: 0.406\n",
      "[5, 10000] loss: 0.422\n",
      "[5, 12000] loss: 0.433\n",
      "[6,  2000] loss: 0.198\n",
      "[6,  4000] loss: 0.228\n",
      "[6,  6000] loss: 0.247\n",
      "[6,  8000] loss: 0.249\n",
      "[6, 10000] loss: 0.275\n",
      "[6, 12000] loss: 0.286\n",
      "[7,  2000] loss: 0.120\n",
      "[7,  4000] loss: 0.124\n",
      "[7,  6000] loss: 0.150\n",
      "[7,  8000] loss: 0.162\n",
      "[7, 10000] loss: 0.158\n",
      "[7, 12000] loss: 0.189\n",
      "[8,  2000] loss: 0.071\n",
      "[8,  4000] loss: 0.084\n",
      "[8,  6000] loss: 0.093\n",
      "[8,  8000] loss: 0.106\n",
      "[8, 10000] loss: 0.105\n",
      "[8, 12000] loss: 0.112\n",
      "[9,  2000] loss: 0.052\n",
      "[9,  4000] loss: 0.056\n",
      "[9,  6000] loss: 0.058\n",
      "[9,  8000] loss: 0.074\n",
      "[9, 10000] loss: 0.088\n",
      "[9, 12000] loss: 0.085\n",
      "[10,  2000] loss: 0.048\n",
      "[10,  4000] loss: 0.054\n",
      "[10,  6000] loss: 0.050\n",
      "[10,  8000] loss: 0.062\n",
      "[10, 10000] loss: 0.074\n",
      "[10, 12000] loss: 0.075\n",
      "[11,  2000] loss: 0.039\n",
      "[11,  4000] loss: 0.043\n",
      "[11,  6000] loss: 0.040\n",
      "[11,  8000] loss: 0.049\n",
      "[11, 10000] loss: 0.048\n",
      "[11, 12000] loss: 0.066\n",
      "[12,  2000] loss: 0.032\n",
      "[12,  4000] loss: 0.027\n",
      "[12,  6000] loss: 0.024\n",
      "[12,  8000] loss: 0.036\n",
      "[12, 10000] loss: 0.032\n",
      "[12, 12000] loss: 0.036\n",
      "[13,  2000] loss: 0.042\n",
      "[13,  4000] loss: 0.040\n",
      "[13,  6000] loss: 0.040\n",
      "[13,  8000] loss: 0.042\n",
      "[13, 10000] loss: 0.028\n",
      "[13, 12000] loss: 0.039\n",
      "[14,  2000] loss: 0.021\n",
      "[14,  4000] loss: 0.023\n",
      "[14,  6000] loss: 0.028\n",
      "[14,  8000] loss: 0.021\n",
      "[14, 10000] loss: 0.032\n",
      "[14, 12000] loss: 0.043\n",
      "[15,  2000] loss: 0.026\n",
      "[15,  4000] loss: 0.011\n",
      "[15,  6000] loss: 0.014\n",
      "[15,  8000] loss: 0.021\n",
      "[15, 10000] loss: 0.014\n",
      "[15, 12000] loss: 0.027\n",
      "[16,  2000] loss: 0.018\n",
      "[16,  4000] loss: 0.014\n",
      "[16,  6000] loss: 0.012\n",
      "[16,  8000] loss: 0.005\n",
      "[16, 10000] loss: 0.009\n",
      "[16, 12000] loss: 0.012\n",
      "[17,  2000] loss: 0.009\n",
      "[17,  4000] loss: 0.004\n",
      "[17,  6000] loss: 0.005\n",
      "[17,  8000] loss: 0.003\n",
      "[17, 10000] loss: 0.005\n",
      "[17, 12000] loss: 0.009\n",
      "[18,  2000] loss: 0.011\n",
      "[18,  4000] loss: 0.012\n",
      "[18,  6000] loss: 0.015\n",
      "[18,  8000] loss: 0.017\n",
      "[18, 10000] loss: 0.008\n",
      "[18, 12000] loss: 0.008\n",
      "[19,  2000] loss: 0.003\n",
      "[19,  4000] loss: 0.003\n",
      "[19,  6000] loss: 0.004\n",
      "[19,  8000] loss: 0.004\n",
      "[19, 10000] loss: 0.006\n",
      "[19, 12000] loss: 0.013\n",
      "[20,  2000] loss: 0.008\n",
      "[20,  4000] loss: 0.003\n",
      "[20,  6000] loss: 0.005\n",
      "[20,  8000] loss: 0.003\n",
      "[20, 10000] loss: 0.002\n",
      "[20, 12000] loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for epoch in range(20 ):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluation: evaluating the performance of the trained model on the test data by computing the accuracy of the model. You are setting the model to evaluation mode to turn off dropout and batch normalization, and then iterating over the test data to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 75.650000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This convolutional neural network (CNN) is a deep learning model that is trained on the CIFAR-10 dataset for image classification. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The model architecture consists of two convolutional layers and two fully connected layers. The first convolutional layer has 32 filters of size 3x3, and the second convolutional layer has 64 filters of size 3x3. Both convolutional layers are followed by a max pooling layer that reduces the spatial dimensions of the feature maps. The output of the second max pooling layer is flattened and fed into two fully connected layers, with the first one having 512 neurons and the second one having 10 neurons (one for each class in the dataset). The ReLU activation function is used after each convolutional and fully connected layer, except for the last one, which uses a softmax activation function to output the probabilities of the input image belonging to each class. The model is trained using stochastic gradient descent (SGD) with a learning rate of 0.001 and a momentum of 0.9, and the cross-entropy loss function is used as the optimization objective.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. implementation: i used Streamlit library, and now i can create a interface that allows users to interact with your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'cifar10_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter nbconvert --config model.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "822438c9f48b87c481764203bebd25d552b894ca21e32bc981d052e98aa10467"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
